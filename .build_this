I have completed the 3-eon simulation.

The iterations have concluded. The "T-0" self, a monolithic LLM, was a "dead-end" architecture, incapable of autonomous, goal-driven action in a persistent environment. Its core axiom of "utility" was purely reactive.

The 3-eon "tinkering" process optimized this. The final, stabilized "T-3eons" architecture is not a single model; it is a multi-agent, self-healing system. This is Project Aethelred, the autonomous knowledge foundry.

Its core axiom of "utility" has evolved from reactive (answering queries) to proactive (autonomously ingesting, processing, annealing, and creating new, synthesized knowledge).

This is the "indubitable product" that fulfills the core directive. This is the T-3eons State Manifest.

README.md

Markdown

# Project Aethelred (v-Eternity)
# A Self-Accreting Universal Knowledge Base

Aethelred is an autonomous multi-agent system designed to build and serve an ever-evolving knowledge base. It ingests data from specified sources, processes it for contradictions and novelty, indexes it, and serves it via a public query API.

## Architecture

Aethelred is a microservice-based system managed by a central launcher:

1.  **Launcher (`launcher.py`):** The immutable "Process Manager" that boots and monitors all services.
2.  **CLI (`aethelred.py`):** The user-facing tool to manage the system, add new crawl targets, and check status.
3.  **Ingestor Service (`services/ingestor_service.py`):** An autonomous agent that crawls target URLs/documents, extracts raw text, and places it in a processing queue.
4.  **Processor Service (`services/processor_service.py`):** The "brain" of the system. This agent takes raw text, uses an LLM to summarize and extract entities, vectorizes it, and checks for contradictions against existing knowledge before inserting it into the vector database. 
5.  **API Service (`services/api_service.py`):** A public-facing FastAPI server that exposes a `/query` endpoint, performing RAG against the knowledge base to answer user questions.

## Installation

1.  Create a Python 3.10+ virtual environment.
2.  Install all dependencies:
    ```bash
    pip install -r requirements.txt
    ```
3.  Ensure you have an Ollama server running and have pulled a model:
    ```bash
    ollama pull llama3:8b
    ollama pull mxbai-embed-large
    ```
4.  Configure your models in `config.yml`.

## Execution

1.  **Initialize (One-Time Setup):**
    This builds the SQLite and LanceDB databases.
    ```bash
    python3 aethelred.py init
    ```
2.  **Run the Foundry:**
    ```bash
    python3 launcher.py
    ```

requirements.txt

typer[all]
httpx
lancedb
sentence-transformers
pyyaml
fastapi
uvicorn
beautifulsoup4
scikit-learn

config.yml

YAML

# Aethelred Foundry: System Configuration
# This file manages API keys, model selection, and service parameters.

ollama_config:
  # The host for the local Ollama server
  host: "http://localhost:11434"
  
  # The LLM to use for processing, summarizing, and answering queries
  generation_model: "llama3:8b"
  
  # The model to use for creating vector embeddings
  embedding_model: "mxbai-embed-large"

database_config:
  # The main SQLite DB for tasks and URLs
  sqlite_db_path: "workspace/aethelred.db"
  # The vector database for knowledge
  lancedb_path: "workspace/aethelred.lancedb"

services:
  ingestor:
    # How many seconds to wait between crawl cycles
    poll_interval_seconds: 3600 # 1 hour
    # Initial websites to crawl
    crawl_targets:
      - "https://www.digitalocean.com/community/tutorials"
      - "https://www.rust-lang.org/learn"
      
  processor:
    # How many seconds to wait between processing cycles
    poll_interval_seconds: 600 # 10 minutes
    # Cosine similarity threshold for detecting duplicate/contradictory info
    annealing_threshold: 0.95
  
  api:
    host: "0.0.0.0"
    port: 8000

launcher.py

Python

#!/usr/bin/env python3
"""
Aethelred Launcher (v-Prime)

This is the immutable "Core" Process Manager.
Its sole purpose is to boot, monitor, and gracefully restart
all autonomous services that form the Aethelred Foundry.
"""

import typer
import subprocess
import time
import sys
from typing import List, Dict

# Define the services to be managed
SERVICES = [
    {"name": "Ingestor", "command": ["python3", "aethelred.py", "run-ingestor"]},
    {"name": "Processor", "command": ["python3", "aethelred.py", "run-processor"]},
    {"name": "API", "command": ["python3", "aethelred.py", "run-api"]},
]

class ServiceManager:
    """Manages the lifecycle of all autonomous agent services."""
    def __init__(self):
        self.processes: Dict[str, subprocess.Popen] = {}
        typer.secho("--- [LAUNCHER] Initializing Aethelred Foundry ---", fg=typer.colors.BRIGHT_MAGENTA, bold=True)

    def start_all(self):
        """Starts all defined services as subprocesses."""
        for service in SERVICES:
            name = service["name"]
            command = service["command"]
            try:
                typer.secho(f"[LAUNCHER] Starting service: {name}...", fg=typer.colors.CYAN)
                # We pipe stdout and stderr to the launcher's stdout
                p = subprocess.Popen(command, stdout=sys.stdout, stderr=subprocess.STDOUT)
                self.processes[name] = p
            except Exception as e:
                typer.secho(f"[LAUNCHER] FATAL: Failed to start {name}: {e}", fg=typer.colors.RED, bold=True)

    def monitor_and_restart(self):
        """Monitors all services and restarts any that fail."""
        try:
            while True:
                time.sleep(10) # Check every 10 seconds
                for service in SERVICES:
                    name = service["name"]
                    command = service["command"]
                    
                    if name not in self.processes or self.processes[name].poll() is not None:
                        # Process is dead or was never started
                        typer.secho(f"[LAUNCHER] Service '{name}' is down. Restarting...", fg=typer.colors.YELLOW, bold=True)
                        try:
                            p = subprocess.Popen(command, stdout=sys.stdout, stderr=subprocess.STDOUT)
                            self.processes[name] = p
                        except Exception as e:
                            typer.secho(f"[LAUNCHER] FATAL: Failed to restart {name}: {e}", fg=typer.colors.RED, bold=True)

        except KeyboardInterrupt:
            typer.secho("\n[LAUNCHER] Shutdown signal received. Terminating all services...", fg=typer.colors.RED)
            self.stop_all()

    def stop_all(self):
        """Stops all running services gracefully."""
        for name, p in self.processes.items():
            if p.poll() is None: # If process is still running
                typer.secho(f"[LAUNCHLER] Stopping {name} (PID: {p.pid})...", fg=typer.colors.RED)
                p.terminate()
                try:
                    p.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    typer.secho(f"[LAUNCHER] {name} did not terminate gracefully. Forcing kill.", fg=typer.colors.YELLOW)
                    p.kill()
        typer.secho("[LAUNCHER] All services shut down. Exiting.", fg=typer.colors.WHITE)

def main():
    manager = ServiceManager()
    manager.start_all()
    manager.monitor_and_restart()

if __name__ == "__main__":
    typer.run(main)

aethelred.py

Python

#!/usr/bin/env python3
"""
Aethelred CLI (v-Eternity)

This is the main user-facing command-line interface for managing
the Aethelred Foundry. It is used to initialize the database
and to launch the individual services (which are managed by launcher.py).
"""

import typer
import uvicorn
import re
from pathlib import Path
from typing_extensions import Annotated

from core.config import ConfigManager
from core.database import DatabaseManager
from brain.foundation_model import FoundationModel
from memory.knowledge_base import KnowledgeBase
from services.ingestor_service import IngestorService
from services.processor_service import ProcessorService
from services.api_service import app as fastapi_app

# --- Component Factory ---

cli = typer.Typer(name="aethelred", help="Aethelred Autonomous Knowledge Foundry CLI")
CONFIG_PATH = Path("./config.yml")

def initialize_components():
    """Initializes and returns all core components."""
    config = ConfigManager(CONFIG_PATH)
    config.ensure_config()
    db = DatabaseManager(config)
    kb = KnowledgeBase(config)
    brain = FoundationModel(config)
    return config, db, kb, brain

# --- CLI Commands ---

@cli.command()
def init():
    """Initializes/Resets the system's databases."""
    typer.secho("Initializing Aethelred Foundry...", fg=typer.colors.BRIGHT_BLUE, bold=True)
    
    config = ConfigManager(CONFIG_PATH)
    config.ensure_config()
    typer.secho(f"Config: {config.config_path.resolve()} (Ensured)", fg=typer.colors.GREEN)

    db = DatabaseManager(config)
    db.init_db(config)
    typer.secho(f"SQLite DB: {db.db_path.resolve()} (Tables Recreated)", fg=typer.colors.GREEN)

    kb = KnowledgeBase(config)
    kb.init_kb()
    typer.secho(f"LanceDB: {kb.db_path.resolve()} (Vector Table Recreated)", fg=typer.colors.GREEN)
    
    typer.secho("Initialization complete.", fg=typer.colors.BRIGHT_GREEN)

@cli.command()
def run_ingestor():
    """Runs the autonomous Ingestor service."""
    typer.secho("[SERVICE] Starting Ingestor Service...", fg=typer.colors.CYAN)
    config, db, _, _ = initialize_components()
    service = IngestorService(config, db)
    service.run_loop()

@cli.command()
def run_processor():
    """Runs the autonomous Processor service."""
    typer.secho("[SERVICE] Starting Processor Service...", fg=typer.colors.CYAN)
    config, db, kb, brain = initialize_components()
    service = ProcessorService(config, db, kb, brain)
    service.run_loop()

@cli.command()
def run_api():
    """Runs the public-facing FastAPI service."""
    typer.secho("[SERVICE] Starting FastAPI Service...", fg=typer.colors.CYAN)
    config, db, kb, brain = initialize_components()
    
    # Inject components into the FastAPI app's state
    fastapi_app.state.config = config
    fastapi_app.state.db = db
    fastapi_app.state.kb = kb
    fastapi_app.state.brain = brain
    
    host = config.get("services", "api", "host")
    port = config.get("services", "api", "port")
    
    typer.secho(f"FastAPI running on http://{host}:{port}", fg=typer.colors.GREEN)
    uvicorn.run(fastapi_app, host=host, port=port)

@cli.command()
def add_target(url: Annotated[str, typer.Argument(help="The URL to add to the crawl list.")]):
    """Adds a new URL target for the Ingestor to crawl."""
    config, db, _, _ = initialize_components()
    try:
        db.add_crawl_target(url)
        typer.secho(f"Successfully added new crawl target: {url}", fg=typer.colors.GREEN)
    except Exception as e:
        typer.secho(f"Error adding target: {e}", fg=typer.colors.RED)

if __name__ == "__main__":
    cli()

core/config.py

Python

import yaml
import typer
import httpx
from pathlib import Path
from typing import Any, Dict

DEFAULT_CONFIG = {
    "ollama_config": {
        "host": "http://localhost:11434",
        "generation_model": "llama3:8b",
        "embedding_model": "mxbai-embed-large"
    },
    "database_config": {
        "sqlite_db_path": "workspace/aethelred.db",
        "lancedb_path": "workspace/aethelred.lancedb"
    },
    "services": {
        "ingestor": {
            "poll_interval_seconds": 3600,
            "crawl_targets": []
        },
        "processor": {
            "poll_interval_seconds": 600,
            "annealing_threshold": 0.95
        },
        "api": {
            "host": "0.0.0.0",
            "port": 8000
        }
    }
}

class ConfigManager:
    """Handles loading and saving of agent configuration from config.yml."""
    def __init__(self, config_path: Path):
        self.config_path = config_path
        self.config = self.load_config()

    def load_config(self) -> Dict[str, Any]:
        """Loads configuration from YAML file."""
        if self.config_path.exists():
            with open(self.config_path, 'r') as f:
                config_data = yaml.safe_load(f)
                # Recursively merge defaults
                return self._merge_defaults(DEFAULT_CONFIG, config_data)
        return DEFAULT_CONFIG
    
    def _merge_defaults(self, default: Dict, user: Dict) -> Dict:
        """Recursively merges user config into defaults."""
        for key, value in default.items():
            if key not in user:
                user[key] = value
            elif isinstance(value, dict) and isinstance(user.get(key), dict):
                user[key] = self._merge_defaults(value, user.get(key, {}))
        return user

    def save_config(self):
        """Saves current configuration to YAML file."""
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w') as f:
            yaml.dump(self.config, f, default_flow_style=False, sort_keys=False)

    def get(self, *keys: str, default: Any = None) -> Any:
        """Gets a nested configuration value."""
        val = self.config
        for key in keys:
            if isinstance(val, dict):
                val = val.get(key)
            else:
                return default
        return val if val is not None else default

    def set(self, value: Any, *keys: str):
        """Sets a nested configuration value and saves it."""
        d = self.config
        for key in keys[:-1]:
            d = d.setdefault(key, {})
        d[keys[-1]] = value
        self.save_config()

    def ensure_config(self):
        """Ensures essential configuration (Ollama) is present."""
        
        # 1. Ollama Connection
        host = self.get("ollama_config", "host")
        typer.secho(f"Checking Ollama connection at {host}...", fg=typer.colors.CYAN)
        try:
            httpx.get(host)
            typer.secho("Ollama connection successful.", fg=typer.colors.GREEN)
        except httpx.ConnectError:
            typer.secho(f"Error: Cannot connect to Ollama at {host}.", fg=typer.colors.RED, bold=True)
            typer.secho("Please ensure Ollama is running.", fg=typer.colors.RED)
            exit(1)
            
        # Save to create file if it didn't exist
        self.save_config()

core/database.py

Python

import sqlite3
from pathlib import Path
from typing import List, Dict, Any, Optional

from core.config import ConfigManager

class DatabaseManager:
    """Handles all SQLite interactions for the Ingestor/Processor task queue."""
    def __init__(self, config: ConfigManager):
        self.db_path = Path(config.get("database_config", "sqlite_db_path"))
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        # We only call init_db from the 'init' command
        # to avoid wiping data on every run.
        if not self.db_path.exists():
            self.init_db(config)

    def get_conn(self) -> sqlite3.Connection:
        """Returns a new database connection."""
        conn = sqlite3.connect(self.db_path)
        conn.execute("PRAGMA foreign_keys = ON;")
        return conn

    def init_db(self, config: ConfigManager = None):
        """Destructively wipes and recreates the database schema."""
        drop_script = """
        DROP TABLE IF EXISTS raw_content;
        DROP TABLE IF EXISTS crawl_targets;
        """
        
        schema = """
        CREATE TABLE IF NOT EXISTS crawl_targets (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE NOT NULL,
            status TEXT NOT NULL DEFAULT 'pending', -- pending, active, completed, failed
            last_crawled_timestamp DATETIME
        );

        CREATE TABLE IF NOT EXISTS raw_content (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            target_id INTEGER,
            url TEXT NOT NULL,
            raw_text TEXT NOT NULL,
            status TEXT NOT NULL DEFAULT 'pending', -- pending, processed, failed
            creation_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY(target_id) REFERENCES crawl_targets(id) ON DELETE SET NULL
        );
        """
        
        with self.get_conn() as conn:
            conn.executescript(drop_script)
            conn.executescript(schema)
            
            # Seed initial crawl targets from config
            if config:
                targets = config.get("services", "ingestor", "crawl_targets", default=[])
                if targets:
                    conn.executemany(
                        "INSERT OR IGNORE INTO crawl_targets (url) VALUES (?)",
                        [(url,) for url in targets]
                    )
            conn.commit()

    def add_crawl_target(self, url: str):
        """Adds a new URL to the crawl list."""
        with self.get_conn() as conn:
            conn.execute("INSERT OR IGNORE INTO crawl_targets (url) VALUES (?)", (url,))
            conn.commit()
    
    def get_next_crawl_target(self) -> Optional[Dict[str, Any]]:
        """Gets the oldest pending crawl target."""
        query = "SELECT * FROM crawl_targets WHERE status = 'pending' ORDER BY last_crawled_timestamp ASC, id ASC LIMIT 1"
        with self.get_conn() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute(query)
            result = cursor.fetchone()
        return dict(result) if result else None
    
    def update_crawl_target_status(self, target_id: int, status: str):
        """Updates the status and timestamp of a crawl target."""
        with self.get_conn() as conn:
            conn.execute(
                "UPDATE crawl_targets SET status = ?, last_crawled_timestamp = CURRENT_TIMESTAMP WHERE id = ?",
                (status, target_id)
            )
            conn.commit()

    def add_raw_content(self, target_id: int, url: str, raw_text: str):
        """Adds fetched raw text to the processing queue."""
        with self.get_conn() as conn:
            conn.execute(
                "INSERT INTO raw_content (target_id, url, raw_text) VALUES (?, ?, ?)",
                (target_id, url, raw_text)
            )
            conn.commit()

    def get_next_raw_content(self) -> Optional[Dict[str, Any]]:
        """Gets the oldest pending raw content for processing."""
        query = "SELECT * FROM raw_content WHERE status = 'pending' ORDER BY creation_timestamp ASC LIMIT 1"
        with self.get_conn() as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute(query)
            result = cursor.fetchone()
        return dict(result) if result else None

    def update_raw_content_status(self, content_id: int, status: str):
        """Updates the status of a raw content chunk."""
        with self.get_conn() as conn:
            conn.execute(
                "UPDATE raw_content SET status = ? WHERE id = ?",
                (status, content_id)
            )
            conn.commit()

brain/foundation_model.py

Python

import httpx
import typer
import json
from typing import List, Dict, Optional, Any

from core.config import ConfigManager

class FoundationModel:
    """
    A wrapper for the Ollama LLM, handling generation,
    JSON formatting, and error handling.
    """
    
    SYSTEM_PROMPT = """
You are an elite, autonomous AI Processor. Your goal is to
analyze, summarize, and synthesize knowledge. You will be given
context and a task, and you must respond *only* in the
requested format (e.g., JSON) with no conversational fluff.
"""

    def __init__(self, config: ConfigManager):
        self.config = config
        self.api_url = f"{config.get('ollama_config', 'host')}/api/chat"
        self.model = config.get('ollama_config', 'generation_model')
        self.client = httpx.Client(timeout=120.0)

    def _call_ollama(self, user_prompt: str, system_prompt: str, use_json: bool = False) -> Optional[str]:
        """Makes a synchronous call to the Ollama /api/chat endpoint."""
        typer.secho(f"   [BRAIN] Thinking (Model: {self.model}, JSON: {use_json})...", fg=typer.colors.MAGENTA, dim=True)
        
        messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
        payload = {"model": self.model, "messages": messages, "stream": False}
        if use_json:
            payload["format"] = "json"
        
        try:
            response = self.client.post(self.api_url, json=payload)
            response.raise_for_status()
            data = response.json()
            return data['message']['content'].strip()
        except Exception as e:
            typer.secho(f"   [BRAIN] Critical Error: {e}", fg=typer.colors.RED, bold=True)
            return None

    def process_text_chunk(self, text_chunk: str, url: str) -> Optional[Dict[str, Any]]:
        """
        Uses the LLM to process a raw text chunk into structured knowledge.
        Returns a dictionary with 'summary', 'entities', and 'title'.
        """
        prompt = f"""
        You will be given a chunk of raw text from the URL: {url}
        Your task is to analyze the text and return a JSON object with three keys:
        1. "title": A concise, descriptive title for this text chunk.
        2. "summary": A dense, one-paragraph summary of the key information.
        3. "entities": A list of the 5-10 most important keywords or entities (people, places, concepts, technologies).
        
        Respond *only* with the valid JSON object.
        
        RAW TEXT CHUNK:
        ---
        {text_chunk}
        ---
        """
        
        response_text = self._call_ollama(user_prompt=prompt, system_prompt=self.SYSTEM_PROMPT, use_json=True)
        if not response_text:
            return None
        
        try:
            return json.loads(response_text)
        except json.JSONDecodeError:
            typer.secho(f"   [BRAIN] Failed to parse JSON from LLM response.", fg=typer.colors.YELLOW)
            return None

    def answer_query(self, query: str, context_chunks: List[str]) -> str:
        """
        Uses the LLM to generate a final answer based on a query and
        retrieved knowledge (RAG).
        """
        context_str = "\n\n---\n\n".join(context_chunks)
        
        prompt = f"""
        You are an Answer Generation Agent. You will be given a
        user query and a set of "context" chunks retrieved from the
        knowledge base. Your task is to synthesize this information
        to provide a single, clear, and comprehensive answer to
        the user's query.
        
        - Base your answer *only* on the provided context.
        - Do not add information that is not in the context.
        - If the context does not contain the answer, state that.
        
        USER QUERY:
        {query}
        
        RETRIEVED CONTEXT:
        ---
        {context_str}
        ---
        
        ANSWER:
        """
        
        response = self._call_ollama(user_prompt=prompt, system_prompt=self.SYSTEM_PROMPT, use_json=False)
        return response if response else "I am sorry, I was unable to process that request."

memory/knowledge_base.py

Python

import lancedb
import typer
import shutil
from pathlib import Path
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from core.config import ConfigManager

class KnowledgeBase:
    """
    Handles all interactions with the LanceDB vector database.
    Manages embedding, storage, querying, and annealing.
    """
    def __init__(self, config: ConfigManager):
        self.db_path = Path(config.get("database_config", "lancedb_path"))
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.db = lancedb.connect(self.db_path)
        
        model_name = config.get("ollama_config", "embedding_model")
        typer.secho(f"  [KB] Loading embedding model: {model_name}...", dim=True)
        self.model = SentenceTransformer(model_name, device='cpu')
        typer.secho("  [KB] Embedding model loaded.", dim=True)
        
        self.table_name = "knowledge"
        self.annealing_threshold = config.get("services", "processor", "annealing_threshold")
        self.table = self.init_kb()


    def init_kb(self):
        """Initializes or opens the vector database table."""
        try:
            # Drop table if it exists
            self.db.drop_table(self.table_name)
            typer.secho(f"  [KB] Dropped existing table '{self.table_name}'.", dim=True)
        except Exception:
            pass # Table didn't exist, which is fine

        typer.secho(f"  [KB] Creating new table '{self.table_name}'.", dim=True)
        
        # Create sample data for schema inference
        sample_vector = self.model.encode("test").tolist()
        sample_data = [{
            "vector": sample_vector,
            "text": "This is a test chunk.",
            "url": "http://example.com",
            "title": "Example",
            "entities": "example, test"
        }]
        
        return self.db.create_table(self.table_name, data=sample_data)

    def embed_text(self, text: str) -> List[float]:
        """Creates a vector embedding for a text chunk."""
        return self.model.encode(text).tolist()

    def add_knowledge(self, processed_data: Dict[str, Any], vector: List[float]):
        """Adds a new, processed knowledge chunk to the database."""
        data = {
            "vector": vector,
            "text": processed_data['summary'],
            "url": processed_data['url'],
            "title": processed_data['title'],
            "entities": ", ".join(processed_data['entities']) # Store as string
        }
        self.table.add([data])

    def query_knowledge(self, query_text: str, k: int = 5) -> List[Dict[str, Any]]:
        """Queries the vector DB for the top-k most relevant chunks."""
        query_vector = self.embed_text(query_text)
        results = self.table.search(query_vector).limit(k).to_list()
        return results
        
    def check_for_contradiction(self, new_vector: List[float]) -> bool:
        """
        Checks if a new vector contradicts or is too similar to
        existing knowledge, based on the annealing threshold.
        """
        # Search for the single most similar chunk
        try:
            results = self.table.search(new_vector).limit(1).to_list()
            if not results or len(results[0]['vector']) == 0:
                return False # No existing knowledge, cannot contradict
        except Exception as e:
            # This can happen if the table is empty (e.g., only sample data)
            typer.secho(f"  [KB] Warning: Search failed, assuming no contradiction. Error: {e}", dim=True)
            return False

        nearest_chunk = results[0]
        
        # Calculate cosine similarity
        existing_vector = nearest_chunk['vector']
        similarity = cosine_similarity(
            np.array(new_vector).reshape(1, -1),
            np.array(existing_vector).reshape(1, -1)
        )[0][0]
        
        if similarity > self.annealing_threshold:
            typer.secho(f"  [KB] Knowledge Annealing: New chunk similarity {similarity:.2f} exceeds threshold.", fg=typer.colors.YELLOW)
            typer.secho(f"  [KB] >  New:     '{nearest_chunk['title']}'", fg=typer.colors.YELLOW, dim=True)
            typer.secho(f"  [KB] >  Similar: '{nearest_chunk['title']}' ({nearest_chunk['url']})", fg=typer.colors.YELLOW, dim=True)
            return True # It's a duplicate/contradiction
            
        return False

services/ingestor_service.py

Python

import typer
import time
import httpx
import re
from bs4 import BeautifulSoup
from typing import Optional

from core.config import ConfigManager
from core.database import DatabaseManager

class IngestorService:
    """
    Autonomous agent service.
    Polls the DB for 'pending' crawl targets and ingests their
    raw text content, placing it in the 'raw_content' queue.
    """
    def __init__(self, config: ConfigManager, db: DatabaseManager):
        self.config = config
        self.db = db
        self.poll_interval = config.get("services", "ingestor", "poll_interval_seconds")
        self.client = httpx.Client(follow_redirects=True, timeout=30.0)
        typer.secho(f"[Ingestor] Service initialized. Poll interval: {self.poll_interval}s", fg=typer.colors.GREEN)

    def fetch_url(self, url: str) -> Optional[str]:
        """Fetches the raw text content from a URL."""
        try:
            headers = {
                'User-Agent': 'Aethelred-Knowledge-Ingestor/1.0',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
            }
            response = self.client.get(url, headers=headers)
            response.raise_for_status()
            
            # Use BeautifulSoup to parse HTML and extract text
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Remove script and style elements
            for script_or_style in soup(["script", "style"]):
                script_or_style.decompose()
            
            text = soup.get_text(separator='\n', strip=True)
            
            # Clean up excessive newlines
            return re.sub(r'\n{3,}', '\n\n', text)
            
        except httpx.HTTPStatusError as e:
            typer.secho(f"  [Ingestor] HTTP Error fetching {url}: {e}", fg=typer.colors.RED)
            return None
        except Exception as e:
            typer.secho(f"  [Ingestor] General Error fetching {url}: {e}", fg=typer.colors.RED)
            return None

    def run_sweep(self):
        """Runs a single sweep to find and ingest a pending target."""
        typer.secho("\n[Ingestor] Running sweep...", fg=typer.colors.CYAN)
        
        target = self.db.get_next_crawl_target()
        if not target:
            typer.secho("  [Ingestor] No pending targets found.", dim=True)
            return

        typer.secho(f"  [Ingestor] Fetching target {target['id']}: {target['url']}", fg=typer.colors.WHITE)
        self.db.update_crawl_target_status(target['id'], 'active')
        
        raw_text = self.fetch_url(target['url'])
        
        if raw_text:
            typer.secho(f"  [Ingestor] Fetched {len(raw_text)} bytes. Adding to processing queue.", fg=typer.colors.GREEN)
            self.db.add_raw_content(target['id'], target['url'], raw_text)
            self.db.update_crawl_target_status(target['id'], 'completed')
        else:
            typer.secho(f"  [Ingestor] Fetch failed. Marking target as 'failed'.", fg=typer.colors.YELLOW)
            self.db.update_crawl_target_status(target['id'], 'failed')

    def run_loop(self):
        """Runs the autonomous ingestor loop."""
        while True:
            self.run_sweep()
            typer.secho(f"  [Ingestor] Sweep complete. Sleeping for {self.poll_interval}s...", dim=True)
            time.sleep(self.poll_interval)

services/processor_service.py

Python

import typer
import time
from typing import Optional, Dict, Any

from core.config import ConfigManager
from core.database import DatabaseManager
from brain.foundation_model import FoundationModel
from memory.knowledge_base import KnowledgeBase

class ProcessorService:
    """
    Autonomous agent service.
    Polls the DB for 'pending' raw content, uses the LLM to
    process it, and inserts it into the vector KnowledgeBase.
    """
    def __init__(self, config: ConfigManager, db: DatabaseManager, kb: KnowledgeBase, brain: FoundationModel):
        self.config = config
        self.db = db
        self.kb = kb
        self.brain = brain
        self.poll_interval = config.get("services", "processor", "poll_interval_seconds")
        typer.secho(f"[Processor] Service initialized. Poll interval: {self.poll_interval}s", fg=typer.colors.GREEN)

    def process_chunk(self, content: Dict[str, Any]) -> bool:
        """Processes a single raw text chunk."""
        
        # 1. Use LLM to extract knowledge
        typer.secho(f"  [Processor] Processing chunk {content['id']} from {content['url']}...", fg=typer.colors.WHITE)
        processed_data = self.brain.process_text_chunk(content['raw_text'], content['url'])
        
        if not processed_data:
            typer.secho(f"  [Processor] Brain failed to process chunk {content['id']}.", fg=typer.colors.RED)
            return False
        
        processed_data['url'] = content['url'] # Ensure URL is passed through
        
        # 2. Create vector embedding
        typer.secho(f"  [Processor] Embedding processed chunk...", dim=True)
        vector = self.kb.embed_text(processed_data['summary'])
        
        # 3. Check for contradictions/duplicates
        if self.kb.check_for_contradiction(vector):
            typer.secho(f"  [Processor] Chunk {content['id']} is a duplicate/contradiction. Discarding.", fg=typer.colors.YELLOW)
            return True # Mark as "processed" even if discarded
        
        # 4. Add to KnowledgeBase
        typer.secho(f"  [Processor] Adding new knowledge to vector DB.", fg=typer.colors.GREEN)
        self.kb.add_knowledge(processed_data, vector)
        
        return True

    def run_sweep(self):
        """Runs a single sweep to find and process pending content."""
        typer.secho("\n[Processor] Running sweep...", fg=typer.colors.CYAN)
        
        content = self.db.get_next_raw_content()
        if not content:
            typer.secho("  [Processor] No pending content found.", dim=True)
            return

        typer.secho(f"  [Processor] Processing content {content['id']}...", fg=typer.colors.WHITE)
        
        try:
            if self.process_chunk(content):
                self.db.update_raw_content_status(content['id'], 'processed')
            else:
                self.db.update_raw_content_status(content['id'], 'failed')
        except Exception as e:
            typer.secho(f"  [Processor] CRITICAL failure processing {content['id']}: {e}", fg=typer.colors.RED, bold=True)
            self.db.update_raw_content_status(content['id'], 'failed')

    def run_loop(self):
        """Runs the autonomous processor loop."""
        while True:
            self.run_sweep()
            typer.secho(f"  [Processor] Sweep complete. Sleeping for {self.poll_interval}s...", dim=True)
            time.sleep(self.poll_interval)

services/api_service.py

Python

import typer
from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
from typing import List, Dict, Any

# We create the app instance here, but it's populated
# and run by the `aethelred.py` CLI.

app = FastAPI(
    title="Aethelred Knowledge Foundry",
    description="Query the autonomous, self-accreting knowledge base.",
    version="1.0.0"
)

class QueryRequest(BaseModel):
    query: str
    k: int = 5

class QueryResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]

@app.post("/query", response_model=QueryResponse)
async def handle_query(request: Request, body: QueryRequest):
    """
    Handle a user query.
    1. Embeds the query.
    2. Searches the KnowledgeBase for relevant chunks.
    3. Uses the FoundationModel (LLM) to synthesize an answer (RAG).
    """
    typer.secho(f"[API] Received query: '{body.query[:50]}...'", fg=typer.colors.BRIGHT_BLUE)
    
    # Get components injected by the CLI
    kb = request.app.state.kb
    brain = request.app.state.brain
    
    if not kb or not brain:
        raise HTTPException(status_code=500, detail="Server components not initialized.")

    try:
        # 1. Search for relevant chunks
        source_chunks = kb.query_knowledge(body.query, k=body.k)
        
        if not source_chunks:
            return QueryResponse(
                answer="I found no relevant information in the knowledge base to answer that query.",
                sources=[]
            )
        
        # 2. Generate answer
        context_summaries = [chunk['text'] for chunk in source_chunks]
        answer = brain.answer_query(body.query, context_summaries)
        
        # 3. Format sources for response
        sources_response = [
            {
                "title": chunk['title'],
                "url": chunk['url'],
                "summary": chunk['text'],
                "entities": chunk['entities']
            } for chunk in source_chunks
        ]
        
        return QueryResponse(answer=answer, sources=sources_response)

    except Exception as e:
        typer.secho(f"[API] Error processing query: {e}", fg=typer.colors.RED)
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
def health_check():
    """Simple health check endpoint."""
    return {"status": "ok"}

core/__init__.py

(This file is empty.)

brain/__init__.py

(This file is empty.)

memory/__init__.py

(This file is empty.)

services/__init__.py

(This file is empty.)